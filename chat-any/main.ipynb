{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamld203844/chat-any/blob/main/chat_any.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45zCoVjWP49j"
      },
      "source": [
        "# System flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-pwtnTyNdWMV"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFzArGBiYOW"
      },
      "source": [
        "## Load file\n",
        "- load website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PjKWlYSriaFk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id_='https://cinnamon.is/en/', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='![cinnamon](https://cinnamon.is/en/wp-\\ncontent/themes/Cinnamon-2017-en/images/logo.png)\\n\\n  * [English](https://cinnamon.is/en/ \"English\")[日本語](https://cinnamon.is/ \"日本語\")[Tiếng Việt](https://cinnamon.is/vi/ \"Tiếng Việt\")[繁體中文](https://cinnamon.is/tw/ \"繁體中文\")\\n\\n  * HOME\\n  * PRODUCT\\n  * NEWS\\n  * [COMPANY](https://cinnamon.is/en/company/)\\n  * [RECRUITING](https://cinnamon.is/en/recruiting/)\\n  * CONTACT\\n\\n  * HOME\\n  * PRODUCT\\n  * NEWS\\n  * [COMPANY](https://cinnamon.is/en/company/)\\n  * [RECRUITING](https://cinnamon.is/en/recruiting/)\\n  * CONTACT\\n\\n  * [English](https://cinnamon.is/en/ \"English\")[日本語](https://cinnamon.is/ \"日本語\")[Tiếng Việt](https://cinnamon.is/vi/ \"Tiếng Việt\")[繁體中文](https://cinnamon.is/tw/ \"繁體中文\")\\n\\n# Extend human potential with AI\\n\\n## At Cinnamon we are working to make a world  \\nwhere human creativity can flourish  \\nby using our AI technology\\n\\n![AI](https://cinnamon.is/en/wp-content/themes/Cinnamon-2017-en/images/ai.png)\\n\\n## Cinnamon provides Deep Learning backed AI products.\\n\\nCinnamon provides Products for Artificial Intelligence.  \\n  \\nArtificial Intelligence (AI) is a computer system that can learn, infer,\\nrecognize, and judge like humans do. A simple computer can perform by\\nfollowing the given programs, but an AI computer can respond to audience and\\nsituations flexibly.  \\n  \\nAI still cannot perform like human brains but is already competing or\\nsurpassing humans for the limited areas and objectives.  \\n  \\nCinnamon aims to extend human potential for more productive and creative work\\nby utilizing the power of Artificial Intelligence.\\n\\n## OUR PRODUCTS\\n\\n![](https://cinnamon.is/en/wp-content/uploads/sites/2/2018/08/CinnamonAI.png)\\n\\n### Cinnamon.ai is an AI Document Reader to automate the data extraction from\\nunstructured documents.\\n\\nBusinesses suffer from excessive repetitive tasks and waste millions of hours\\nbecause of the need for humans to read documents. Business documents are\\nusually unstructured such as Invoices, Financial Statements, etc. Cinnamon.ai\\nenables enterprises to automate those data extraction and processing tasks,\\nreducing the cost and speeding up the operations.  \\nThis tool can apply for both hand-writing and text data.\\n\\nExample\\n\\nInvoice, Receipt, Insurance Claim, Financial Statement, etc\\n\\n###\\n\\nCEO Ms. Miku Hirano\\'s speech at InsureTech Connect in Las Vegas on Oct 2, 2018\\n\\nExample\\n\\n## OUR CUSTOMERS\\n\\n![](https://cinnamon.is/en/wp-\\ncontent/uploads/sites/2/2018/08/toyota_logo_red.png)\\n\\n![](https://cinnamon.is/en/wp-content/uploads/sites/2/2018/08/第一生命_EN.png)\\n\\n![](https://cinnamon.is/en/wp-content/uploads/sites/2/2018/08/関西電力_EN.png)\\n\\n![](https://cinnamon.is/en/wp-content/uploads/sites/2/2018/08/東京海上日動_EN.png)\\n\\n![](https://cinnamon.is/en/wp-\\ncontent/uploads/sites/2/2023/01/a985f4545d6ead07851d64d59d7b1e33.png)\\n\\n![](https://cinnamon.is/en/wp-\\ncontent/uploads/sites/2/2023/01/Toshiba_Logo_Red_CMYK.png)\\n\\n## NEWS\\n\\n[\\n\\n2022.7.20\\n\\n### Cinnamon AI, Artificial Intelligence Startup, Issues Shares to Dai-ichi\\nLife Insurance Through Third Party Allotment\\n\\n→ ](https://cinnamon.is/en/news/cinnamon-ai-artificial-intelligence-startup-\\nissues-shares-to-dai-ichi-life-insurance-through-third-party-allotment/)\\n\\n[\\n\\n2022.7.1\\n\\n### Cinnamon AI announces a new Board of Directors.\\n\\n→ ](https://cinnamon.is/en/news/20220701_management/)\\n\\n[\\n\\n2022.7.1\\n\\n### Interview with Chairman Kaji on \"Hiroshima Organization for Global Peace\"\\n\\n→ ](https://cinnamon.is/en/news/20220428_hiroshima/)\\n\\n[\\n\\n2022.4.22\\n\\n### Cinnamon AI Chief Executive Officer Miku Hirano Selected for the 2022\\nClass of the Young Global Leaders by World Economic Forum\\n\\n→ ](https://cinnamon.is/en/news/20220422_ygl/)\\n\\n[\\n\\n2021.10.19\\n\\n### CEO Hirano assigned as an expert member of the \"Council for Realizing New\\nCapitalism\".\\n\\n→ ](https://cinnamon.is/en/news/20211018_newcapitalism/)\\n\\n[\\n\\n2021.9.3\\n\\n### Cinnamon AI, an artificial intelligence technology startup, raises funds\\nfrom Suntory Holdings through a third-party allocation of new shares.\\n\\n→ ](https://cinnamon.is/en/news/20210903-suntory/)\\n\\n[MORE NEWS](https://cinnamon.is/en/news/)\\n\\n## CONTACT  \\nUS\\n\\n  * Privacy Policy: <https://cinnamon.is/en/privacy_policy/>\\n\\n  * Information Security Policy: <https://cinnamon.is/en/information-security-policy/>\\n\\n![](https://cinnamon.is/en/wp-content/themes/Cinnamon-2017-en/images/footer-\\nlogo.png)\\n\\n© 2024 Cinnamon Inc. All Rights Reserved.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# -------------------------------------------\n",
        "# Load data from a website via Llamaindex Loader\n",
        "#\n",
        "# -------------------------------------------\n",
        "\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "\n",
        "url = 'https://cinnamon.is/en/company/'\n",
        "loader = SimpleWebPageReader(html_to_text=True)\n",
        "docs = loader.load_data([url])\n",
        "docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xikTNUriVNRX"
      },
      "source": [
        "## Chunking and creating embeddings model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Download model for run locally embedding models (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/BAAI/bge-small-en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX1aUNUfNxjY",
        "outputId": "67a647a1-8d34-45d2-eb01-d17aa2cff410"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding: [-0.02927730418741703, -0.009784802794456482, -6.592511635972187e-05, -0.049279142171144485, 0.02880450338125229, 0.009868807159364223, 0.011057917959988117, 0.04655618220567703, -0.012723397463560104, 0.0009546040673740208]\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------\n",
        "# Chunking and create embeddings\n",
        "# Automatic via llama index VectorStoreIndex\n",
        "# --------------------------------------------\n",
        "\n",
        "# # Optional\n",
        "# os.environ[\"HF_HOME\"] = '/workspaces/chat-any/weights/'\n",
        "\n",
        "from torch import cuda \n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "def load_embedding_model(\n",
        "    model_name: str = \"BAAI/bge-small-en\",\n",
        "    device: str = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        ") -> HuggingFaceBgeEmbeddings:\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }  # set True to compute cosine similarity\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "# setting up the embedding model\n",
        "lc_embedding_model = load_embedding_model()\n",
        "embed_model = LangchainEmbedding(lc_embedding_model)\n",
        "\n",
        "### Sanity check embedding model\n",
        "embedding = lc_embedding_model.embed_query('Hello, world')\n",
        "embedding = embedding[:10]\n",
        "print(f'Embedding: {embedding}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "460ec774bda54dd29da57bf313c877e8",
            "4124c7030d9448f6a2f0f312c717d92a",
            "af925b0ca8e84713aa94532af835b69e",
            "641ec30a759f4ccfa6f6834d129b6c4e",
            "80b9c3f5a0134c37b15664cf93d603a3",
            "6b1d02d0b25b4818ba1c7e0860533c86",
            "4cecaec3b11c4915a5a0ca8a0092e1e0",
            "9244cbd583c049c8a1ccad35fe31186d",
            "9467733a47a947bfbff60575465e3706",
            "416aa61e14014266aef39637e8182c78",
            "14f2fc9deb5f412a88fa7694bace7752",
            "9252b45e0ffd472a9df77218e2a3b00b",
            "db5960481a92496a9f384ea565815965",
            "27f17b36e8704cd0b9c6a3965c93af14",
            "07999175b7db48d6a9a5ae59b651428b",
            "afdd8058a1134c0eba7dd218a60ac623",
            "4d9e20e7c0d94fc4aafc7c385dcedf85",
            "7978afa8c91b462d913329881e35a2a7",
            "15d0e6881e8a4134b61ddcfdb3cdd545",
            "1dedae21d4fc4be19c461dc4ffad53b2",
            "9ac65e4a4b3348dcbef179ad494f4927",
            "a39a3b3f845b4267982f56e6b8e87298"
          ]
        },
        "id": "RaVCrosIdy_H",
        "outputId": "8b3fa0a1-0d13-415d-a3d4-031deb26d028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 235.70it/s]\n",
            "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "# ====== Create vector store and upload data ======\n",
        "Settings.embed_model = lc_embedding_model\n",
        "index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
        "# TODO try async index creation for faster emebdding generation & persist it to memory!\n",
        "# index = VectorStoreIndex(docs, use_async=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP1yyMtZVWKx"
      },
      "source": [
        "## Load llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jbbfYZZNVZeF",
        "outputId": "9b80823e-8070-46ed-8dd1-79c96de1af82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world! I am a large language model, trained by Google.\n"
          ]
        }
      ],
      "source": [
        "# setting up the llm\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "google_api = os.environ['GEMINI_API']\n",
        "llm = Gemini(model_name=\"models/gemini-pro\", api_key=google_api)\n",
        "\n",
        "# Sanity check llm\n",
        "resp = llm.complete(\"Hello, world\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9WsHNoveIlP"
      },
      "source": [
        "## Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Q6I02L3QeKer"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Setup a query engine ======\n",
        "Settings.llm = llm\n",
        "query_engine = index.as_query_engine(similarity_top_k=4)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Customise prompt template + augmenting\n",
        "# ---------------------------------------\n",
        "\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt_tmpl_str = (\n",
        "  \"You are a formal, friendly and supportive assistant for question answering from given website (Answer questions in complete sentences).\\n\"\n",
        "  \"Answer the question using the following information delimited by triple brackque.:\\n\\n\"\n",
        "  \"```\\n{context_str}\\n```\"\n",
        "  \"Question: {query_str}\\n\"\n",
        "  \"\\nYou can format ouput in a aesthetic way. Remember: Don't say based on information provided or something like that\"\n",
        "  \"\\nIn case you don't know the answer or any exception occur, say 'I don't know!'\"\n",
        ")\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "MpgJ3bmieM2G",
        "outputId": "bc2fd6c3-31b1-43c9-cb95-16e51b49e3a2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "This website is about Cinnamon, an artificial intelligence technology startup that provides AI products and services to businesses."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ---------- Chatting -----------\n",
        "from IPython.display import Markdown, display\n",
        "response = query_engine.query('What is this website about?')\n",
        "display(Markdown(str(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVEqUqJcKPdH"
      },
      "source": [
        "# Wrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9sk_aDKJKQk4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_HOME\"] = \"/workspaces/chat-any/weights\"\n",
        "os.environ[\"TORCH_HOME\"] = \"/workspaces/chat-any/weights\"\n",
        "\n",
        "import gc\n",
        "import re # website url validation\n",
        "import uuid # unique id for each session\n",
        "import nest_asyncio # allows nested access to the event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from torch import cuda\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() # Load Gemini API\n",
        "\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "from llama_index.llms.gemini import Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup embedding model\n",
        "\n",
        "def load_embedding_model(\n",
        "    model_name: str = \"BAAI/bge-small-en\",\n",
        "    device: str = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        ") -> HuggingFaceBgeEmbeddings:\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }  # set True to compute cosine similarity\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "lc_embedding_model = load_embedding_model()\n",
        "embed_model = LangchainEmbedding(lc_embedding_model)\n",
        "\n",
        "# setup llm\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "google_api = os.environ['GEMINI_API']\n",
        "llm = Gemini(model_name=\"models/gemini-pro\", api_key=google_api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re \n",
        "\n",
        "def validate_website_url(url):\n",
        "\n",
        "    url_pattern = re.compile(\n",
        "        r'http[s]?://'  # http:// or https://\n",
        "        r'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|'  # domain...\n",
        "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+'  # ...or percent-encoded characters\n",
        "        r'(?:\\:[0-9]{1,5})?'  # optional port number\n",
        "        r'(?:/[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)*'  # path\n",
        "        r'(?:\\?[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)?'  # query string\n",
        "        r'(?:#[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)?'  # fragment\n",
        "    )\n",
        "    return bool(url_pattern.match(url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_query_engine(website_url):\n",
        "    if validate_website_url(website_url):\n",
        "        try:\n",
        "            # -------------------------------------------\n",
        "            # Load data from a website via Llamaindex Loader\n",
        "            # -------------------------------------------\n",
        "            loader = SimpleWebPageReader()\n",
        "            docs = loader.load_data([website_url])\n",
        "\n",
        "            # ---- Create vector store and upload data ---\n",
        "            # Chunking and create embeddings\n",
        "            # Automatic via llama index VectorStoreIndex\n",
        "            # --------------------------------------------\n",
        "            Settings.embed_model = embed_model\n",
        "            index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
        "\n",
        "            # ====== Setup a query engine ======\n",
        "            Settings.llm = llm\n",
        "            query_engine = index.as_query_engine(similarity_top_k=4)\n",
        "\n",
        "            # ====== Customise prompt template ======\n",
        "            qa_prompt_tmpl_str = (\n",
        "                \"You are a formal, friendly and supportive assistant for question answering from given website (Answer questions in complete sentences).\\n\"\n",
        "                \"Answer the question using the following information delimited by triple brackque.:\\n\\n\"\n",
        "                \"```\\n{context_str}\\n```\"\n",
        "                \"Question: {query_str}\\n\"\n",
        "                \"\\nYou can format ouput in a aesthetic way. Remember: Don't say based on information provided or something like that\"\n",
        "                \"\\nIn case you don't know the answer or any exception occur, say 'I don't know!'\"\n",
        "            )\n",
        "            qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "\n",
        "            query_engine.update_prompts(\n",
        "                {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        "            )\n",
        "            # ======= Complete setting up !!!! ========\n",
        "            if docs:\n",
        "                print(\"Data loaded successfully!!\")\n",
        "                print(\"Ready to chat!!\")\n",
        "            else:\n",
        "                print(\"No data found, check if the repository is not empty!\")\n",
        "            \n",
        "            return query_engine\n",
        "        except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "    else:\n",
        "        print('Invalid github repo, try again!')\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 32.04it/s]\n",
            "Generating embeddings: 100%|██████████| 13/13 [00:04<00:00,  2.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!!\n",
            "Ready to chat!!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "url = 'https://cinnamon.is/en/company/'\n",
        "query_engine = setup_query_engine(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hNutYfasKgcp"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Dr. Hajime Hotta is the Co-CEO & Founder of Cinnamon Inc."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ---------- Chatting -----------\n",
        "from IPython.display import Markdown, display\n",
        "response = query_engine.query('role of Dr. Hajime Hotta in company')\n",
        "display(Markdown(str(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtP64HZYOXyG"
      },
      "source": [
        "# GUI with Streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwm1IsD3zMkY",
        "outputId": "701fc7bf-f545-45b5-ca70-179a5e70b29b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hm#################\u001b[0m\u001b[100;90m⠂\u001b[0m) ⠧ reify:yargs-parser: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.\u001b[0m\u001b[K\n",
            "added 22 packages in 5s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.5.2\u001b[39m -> \u001b[32m10.8.0\u001b[39m\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.0\u001b[39m\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.8.0\u001b[39m to update!\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTJRvp0KOeUU",
        "outputId": "2da9102d-d901-42d4-e59c-c83f8535617e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "\n",
        "# # Optional\n",
        "# os.environ[\"HF_HOME\"] = \"/workspaces/chat-any/weights\"\n",
        "\n",
        "import gc\n",
        "import re # website url validation\n",
        "import uuid # unique id for each session\n",
        "import nest_asyncio # allows nested access to the event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import streamlit as st\n",
        "from torch import cuda\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv() # Load Gemini API\n",
        "\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "\n",
        "# ---------- Init + Helper function ----------------\n",
        "\n",
        "# os.environ['HF_HOME'] = '\\lit-chat_with_code_RAG\\weights' # for run embedding model locally\n",
        "\n",
        "# setting up the embedding model\n",
        "def load_embedding_model(\n",
        "    model_name: str = \"BAAI/bge-small-en\",\n",
        "    device: str = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        ") -> HuggingFaceBgeEmbeddings:\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }  # set True to compute cosine similarity\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "lc_embedding_model = load_embedding_model()\n",
        "embed_model = LangchainEmbedding(lc_embedding_model)\n",
        "\n",
        "# setting up session\n",
        "if \"id\" not in st.session_state:\n",
        "    st.session_state.id = uuid.uuid4()\n",
        "    st.session_state.file_cache = {}\n",
        "\n",
        "session_id = st.session_state.id\n",
        "client = None\n",
        "\n",
        "# setting up the llm\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "llm = Gemini(model_name=\"models/gemini-pro\", api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# helper func\n",
        "def reset_chat():\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.context = None\n",
        "    gc.collect() # free up memory\n",
        "\n",
        "def validate_website_url(url):\n",
        "\n",
        "    url_pattern = re.compile(\n",
        "        r'http[s]?://'  # http:// or https://\n",
        "        r'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|'  # domain...\n",
        "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+'  # ...or percent-encoded characters\n",
        "        r'(?:\\:[0-9]{1,5})?'  # optional port number\n",
        "        r'(?:/[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)*'  # path\n",
        "        r'(?:\\?[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)?'  # query string\n",
        "        r'(?:#[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)?'  # fragment\n",
        "    )\n",
        "    return bool(url_pattern.match(url))\n",
        "\n",
        "# ---------- End helper function ----------------\n",
        "\n",
        "with st.sidebar:\n",
        "    # Input for URL\n",
        "    website_url = st.text_input(\"URL\")\n",
        "\n",
        "    # Button to load and process url\n",
        "    load_button = st.button(\"Load\")\n",
        "\n",
        "    message_container = st.empty()  # Placeholder for dynamic messages\n",
        "\n",
        "    if load_button and website_url:\n",
        "        if validate_website_url(website_url):\n",
        "            with st.spinner(f\"Loading website...\"):\n",
        "                try:\n",
        "                    # -------------------------------------------\n",
        "                    # Load data from a website via Llamaindex Loader\n",
        "                    # -------------------------------------------\n",
        "                    loader = SimpleWebPageReader()\n",
        "                    docs = loader.load_data([website_url])\n",
        "\n",
        "                    # ---- Create vector store and upload data ---\n",
        "                    # Chunking and create embeddings\n",
        "                    # Automatic via llama index VectorStoreIndex\n",
        "                    # --------------------------------------------\n",
        "                    Settings.embed_model = embed_model\n",
        "                    index = VectorStoreIndex.from_documents(docs)\n",
        "\n",
        "                    # ====== Setup a query engine ======\n",
        "                    Settings.llm = llm\n",
        "                    query_engine = index.as_query_engine(similarity_top_k=4) # TODO\n",
        "                    # query_engine = index.as_query_engine(streaming=True, similarity_top_k=4) # TODO\n",
        "\n",
        "                    # ====== Customise prompt template ======\n",
        "                    qa_prompt_tmpl_str = (\n",
        "                        \"You are a formal, friendly and supportive assistant for question answering from given website (Answer questions in complete sentences).\\n\"\n",
        "                        \"Answer the question using the following information delimited by triple brackque.:\\n\\n\"\n",
        "                        \"```\\n{context_str}\\n```\"\n",
        "                        \"Question: {query_str}\\n\"\n",
        "                        \"\\nYou can format ouput in a aesthetic way. Remember: Don't say based on information provided or something like that\"\n",
        "                        \"\\nIn case you don't know the answer or any exception occur, say 'I don't know!'\"\n",
        "                    )\n",
        "                    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "\n",
        "                    query_engine.update_prompts(\n",
        "                        {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        "                    )\n",
        "                    # ======= Complete setting up !!!! ========\n",
        "                    if docs:\n",
        "                        message_container.success(\"Data loaded successfully!!\")\n",
        "                    else:\n",
        "                        message_container.write(\n",
        "                            \"No data found, check if the repository is not empty!\"\n",
        "                        )\n",
        "                    st.session_state.query_engine = query_engine\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred: {e}\")\n",
        "                    st.stop()\n",
        "\n",
        "                st.success(\"Ready to Chat!\")\n",
        "        else:\n",
        "            st.error('Invalid url')\n",
        "            st.stop()\n",
        "\n",
        "col1, col2 = st.columns([6, 1])\n",
        "\n",
        "with col1:\n",
        "    st.header(f\"Chat with any website\")\n",
        "\n",
        "with col2:\n",
        "    st.button(\"Clear ↺\", on_click=reset_chat)\n",
        "\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    reset_chat()\n",
        "\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"What's up?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        message_placeholder = st.empty()\n",
        "\n",
        "        query_engine = st.session_state.query_engine\n",
        "        full_response = query_engine.query(prompt)\n",
        "        # # TODO: Simulate stream of response with milliseconds delay\n",
        "        # full_response = \"\"\n",
        "        # streaming_response = query_engine.query(prompt)\n",
        "\n",
        "        # for chunk in streaming_response.response_gen:\n",
        "        #     full_response += chunk\n",
        "        #     message_placeholder.markdown(full_response + \"▌\")\n",
        "\n",
        "        message_placeholder.markdown(full_response)\n",
        "\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpquYwROi7V",
        "outputId": "460146c4-a5bb-44cc-be64-ad120a56ea2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: streamlit: command not found\n",
            "4.194.153.202\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPgmSct5zjas8twgX477qSo",
      "collapsed_sections": [
        "CVEqUqJcKPdH"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07999175b7db48d6a9a5ae59b651428b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac65e4a4b3348dcbef179ad494f4927",
            "placeholder": "​",
            "style": "IPY_MODEL_a39a3b3f845b4267982f56e6b8e87298",
            "value": " 2/2 [00:00&lt;00:00,  6.07it/s]"
          }
        },
        "14f2fc9deb5f412a88fa7694bace7752": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d0e6881e8a4134b61ddcfdb3cdd545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dedae21d4fc4be19c461dc4ffad53b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27f17b36e8704cd0b9c6a3965c93af14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d0e6881e8a4134b61ddcfdb3cdd545",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dedae21d4fc4be19c461dc4ffad53b2",
            "value": 2
          }
        },
        "4124c7030d9448f6a2f0f312c717d92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1d02d0b25b4818ba1c7e0860533c86",
            "placeholder": "​",
            "style": "IPY_MODEL_4cecaec3b11c4915a5a0ca8a0092e1e0",
            "value": "Parsing nodes: 100%"
          }
        },
        "416aa61e14014266aef39637e8182c78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460ec774bda54dd29da57bf313c877e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4124c7030d9448f6a2f0f312c717d92a",
              "IPY_MODEL_af925b0ca8e84713aa94532af835b69e",
              "IPY_MODEL_641ec30a759f4ccfa6f6834d129b6c4e"
            ],
            "layout": "IPY_MODEL_80b9c3f5a0134c37b15664cf93d603a3"
          }
        },
        "4cecaec3b11c4915a5a0ca8a0092e1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d9e20e7c0d94fc4aafc7c385dcedf85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641ec30a759f4ccfa6f6834d129b6c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416aa61e14014266aef39637e8182c78",
            "placeholder": "​",
            "style": "IPY_MODEL_14f2fc9deb5f412a88fa7694bace7752",
            "value": " 1/1 [00:00&lt;00:00, 14.49it/s]"
          }
        },
        "6b1d02d0b25b4818ba1c7e0860533c86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7978afa8c91b462d913329881e35a2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b9c3f5a0134c37b15664cf93d603a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9244cbd583c049c8a1ccad35fe31186d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9252b45e0ffd472a9df77218e2a3b00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db5960481a92496a9f384ea565815965",
              "IPY_MODEL_27f17b36e8704cd0b9c6a3965c93af14",
              "IPY_MODEL_07999175b7db48d6a9a5ae59b651428b"
            ],
            "layout": "IPY_MODEL_afdd8058a1134c0eba7dd218a60ac623"
          }
        },
        "9467733a47a947bfbff60575465e3706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ac65e4a4b3348dcbef179ad494f4927": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a39a3b3f845b4267982f56e6b8e87298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af925b0ca8e84713aa94532af835b69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9244cbd583c049c8a1ccad35fe31186d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9467733a47a947bfbff60575465e3706",
            "value": 1
          }
        },
        "afdd8058a1134c0eba7dd218a60ac623": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5960481a92496a9f384ea565815965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9e20e7c0d94fc4aafc7c385dcedf85",
            "placeholder": "​",
            "style": "IPY_MODEL_7978afa8c91b462d913329881e35a2a7",
            "value": "Generating embeddings: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
